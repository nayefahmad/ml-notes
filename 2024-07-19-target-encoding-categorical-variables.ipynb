{"metadata":{"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1828856,"sourceType":"datasetVersion","datasetId":933090}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/target-encoding).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn this exercise, you'll apply target encoding to features in the [*Ames*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) dataset.\n\nRun this cell to set everything up!","metadata":{}},{"cell_type":"code","source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.feature_engineering_new.ex6 import *\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\nwarnings.filterwarnings('ignore')\n\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n\ndf = pd.read_csv(\"../input/fe-course-data/ames.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T20:15:58.507674Z","iopub.execute_input":"2024-10-29T20:15:58.508089Z","iopub.status.idle":"2024-10-29T20:16:00.578573Z","shell.execute_reply.started":"2024-10-29T20:15:58.508038Z","shell.execute_reply":"2024-10-29T20:16:00.577578Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3788892820.py:16: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-whitegrid\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------\n\nFirst you'll need to choose which features you want to apply a target encoding to. Categorical features with a large number of categories are often good candidates. Run this cell to see how many categories each categorical feature in the *Ames* dataset has.","metadata":{}},{"cell_type":"code","source":"df.select_dtypes([\"object\"]).nunique().sort_values()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T20:16:44.657580Z","iopub.execute_input":"2024-10-29T20:16:44.658039Z","iopub.status.idle":"2024-10-29T20:16:44.682991Z","shell.execute_reply.started":"2024-10-29T20:16:44.657990Z","shell.execute_reply":"2024-10-29T20:16:44.681660Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Street            2\nCentralAir        2\nAlley             3\nUtilities         3\nLandSlope         3\nPavedDrive        3\nLotShape          4\nLandContour       4\nExterQual         4\nMasVnrType        4\nGarageFinish      4\nExterCond         5\nHeatingQC         5\nKitchenQual       5\nBldgType          5\nPoolQC            5\nFence             5\nLotConfig         5\nMiscFeature       5\nBsmtExposure      5\nGarageCond        6\nFireplaceQu       6\nHeating           6\nElectrical        6\nGarageQual        6\nSaleCondition     6\nBsmtQual          6\nBsmtCond          6\nFoundation        6\nRoofStyle         6\nBsmtFinType1      7\nGarageType        7\nMSZoning          7\nBsmtFinType2      7\nRoofMatl          8\nFunctional        8\nHouseStyle        8\nCondition2        8\nOverallCond       9\nCondition1        9\nSaleType         10\nOverallQual      10\nExterior1st      16\nMSSubClass       16\nExterior2nd      17\nNeighborhood     28\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"We talked about how the M-estimate encoding uses smoothing to improve estimates for rare categories. To see how many times a category occurs in the dataset, you can use the `value_counts` method. This cell shows the counts for `SaleType`, but you might want to consider others as well.","metadata":{}},{"cell_type":"code","source":"df[\"SaleType\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T20:17:59.963481Z","iopub.execute_input":"2024-10-29T20:17:59.964270Z","iopub.status.idle":"2024-10-29T20:17:59.973900Z","shell.execute_reply.started":"2024-10-29T20:17:59.964230Z","shell.execute_reply":"2024-10-29T20:17:59.972783Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"SaleType\nWD       2536\nNew       239\nCOD        87\nConLD      26\nCWD        12\nConLI       9\nConLw       8\nOth         7\nCon         5\nVWD         1\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df['Neighborhood'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T20:18:17.890695Z","iopub.execute_input":"2024-10-29T20:18:17.891089Z","iopub.status.idle":"2024-10-29T20:18:17.900338Z","shell.execute_reply.started":"2024-10-29T20:18:17.891054Z","shell.execute_reply":"2024-10-29T20:18:17.899301Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Neighborhood\nNorth_Ames                                 443\nCollege_Creek                              267\nOld_Town                                   239\nEdwards                                    194\nSomerset                                   182\nNorthridge_Heights                         166\nGilbert                                    165\nSawyer                                     151\nNorthwest_Ames                             131\nSawyer_West                                125\nMitchell                                   114\nBrookside                                  108\nCrawford                                   103\nIowa_DOT_and_Rail_Road                      93\nTimberland                                  72\nNorthridge                                  71\nStone_Brook                                 51\nSouth_and_West_of_Iowa_State_University     48\nClear_Creek                                 44\nMeadow_Village                              37\nBriardale                                   30\nBloomington_Heights                         28\nVeenker                                     24\nNorthpark_Villa                             23\nBlueste                                     10\nGreens                                       8\nGreen_Hills                                  2\nLandmark                                     1\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df['Exterior1st'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T20:20:56.296729Z","iopub.execute_input":"2024-10-29T20:20:56.297144Z","iopub.status.idle":"2024-10-29T20:20:56.310857Z","shell.execute_reply.started":"2024-10-29T20:20:56.297104Z","shell.execute_reply":"2024-10-29T20:20:56.309558Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Exterior1st\nVinylSd    1026\nMetalSd     450\nHdBoard     442\nWd Sdng     420\nPlywood     221\nCemntBd     126\nBrkFace      88\nWdShing      56\nAsbShng      44\nStucco       43\nBrkComm       6\nAsphShn       2\nCBlock        2\nStone         2\nPreCast       1\nImStucc       1\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# 1) Choose Features for Encoding\n\nWhich features did you identify for target encoding? After you've thought about your answer, run the next cell for some discussion.","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_1.check()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T20:21:19.828457Z","iopub.execute_input":"2024-10-29T20:21:19.829478Z","iopub.status.idle":"2024-10-29T20:21:19.838454Z","shell.execute_reply.started":"2024-10-29T20:21:19.829429Z","shell.execute_reply":"2024-10-29T20:21:19.837138Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"1_Q1\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: \n\n The `Neighborhood` feature looks promising. It has the most categories of any feature, and several categories are rare. Others that could be worth considering are `SaleType`, `MSSubClass`, `Exterior1st`, `Exterior2nd`. In fact, almost any of the nominal features would be worth trying because of the prevalence of rare categories.","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> \n\n The `Neighborhood` feature looks promising. It has the most categories of any feature, and several categories are rare. Others that could be worth considering are `SaleType`, `MSSubClass`, `Exterior1st`, `Exterior2nd`. In fact, almost any of the nominal features would be worth trying because of the prevalence of rare categories."},"metadata":{}}]},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------\n\nNow you'll apply a target encoding to your choice of feature. As we discussed in the tutorial, to avoid overfitting, we need to fit the encoder on data heldout from the training set. Run this cell to create the encoding and training splits:","metadata":{}},{"cell_type":"code","source":"# Encoding split\nX_encode = df.sample(frac=0.20, random_state=0)\ny_encode = X_encode.pop(\"SalePrice\")\n\n# Training split\nX_pretrain = df.drop(X_encode.index)\ny_train = X_pretrain.pop(\"SalePrice\")","metadata":{"lines_to_next_cell":2},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Apply M-Estimate Encoding\n\nApply a target encoding to your choice of categorical features. Also choose a value for the smoothing parameter `m` (any value is okay for a correct answer).","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE: Create the MEstimateEncoder\n# Choose a set of features to encode and a value for m\nencoder = ____\n\n\n# Fit the encoder on the encoding split\n____\n\n# Encode the training split\nX_train = encoder.transform(X_pretrain, y_train)\n\n\n# Check your answer\nq_2.check()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you'd like to see how the encoded feature compares to the target, you can run this cell:","metadata":{}},{"cell_type":"code","source":"feature = encoder.cols\n\nplt.figure(dpi=90)\nax = sns.distplot(y_train, kde=True, hist=False)\nax = sns.distplot(X_train[feature], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\nax.set_xlabel(\"SalePrice\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the distribution plots, does it seem like the encoding is informative?\n\nAnd this cell will show you the score of the encoded set compared to the original set:","metadata":{}},{"cell_type":"code","source":"X = df.copy()\ny = X.pop(\"SalePrice\")\nscore_base = score_dataset(X, y)\nscore_new = score_dataset(X_train, y_train)\n\nprint(f\"Baseline Score: {score_base:.4f} RMSLE\")\nprint(f\"Score with Encoding: {score_new:.4f} RMSLE\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do you think that target encoding was worthwhile in this case? Depending on which feature or features you chose, you may have ended up with a score significantly worse than the baseline. In that case, it's likely the extra information gained by the encoding couldn't make up for the loss of data used for the encoding.","metadata":{}},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------\n\nIn this question, you'll explore the problem of overfitting with target encodings. This will illustrate this importance of training fitting target encoders on data held-out from the training set.\n\nSo let's see what happens when we fit the encoder and the model on the *same* dataset. To emphasize how dramatic the overfitting can be, we'll mean-encode a feature that should have no relationship with `SalePrice`, a count: `0, 1, 2, 3, 4, 5, ...`.","metadata":{}},{"cell_type":"code","source":"# Try experimenting with the smoothing parameter m\n# Try 0, 1, 5, 50\nm = 0\n\nX = df.copy()\ny = X.pop('SalePrice')\n\n# Create an uninformative feature\nX[\"Count\"] = range(len(X))\nX[\"Count\"][1] = 0  # actually need one duplicate value to circumvent error-checking in MEstimateEncoder\n\n# fit and transform on the same dataset\nencoder = MEstimateEncoder(cols=\"Count\", m=m)\nX = encoder.fit_transform(X, y)\n\n# Results\nscore =  score_dataset(X, y)\nprint(f\"Score: {score:.4f} RMSLE\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost a perfect score!","metadata":{}},{"cell_type":"code","source":"plt.figure(dpi=90)\nax = sns.distplot(y, kde=True, hist=False)\nax = sns.distplot(X[\"Count\"], color='r', ax=ax, hist=True, kde=False, norm_hist=True)\nax.set_xlabel(\"SalePrice\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the distributions are almost exactly the same, too.\n\n# 3) Overfitting with Target Encoders\n\nBased on your understanding of how mean-encoding works, can you explain how XGBoost was able to get an almost a perfect fit after mean-encoding the count feature?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_3.check()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uncomment this if you'd like a hint before seeing the answer\n#q_3.hint()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The End #\n\nThat's it for *Feature Engineering*! We hope you enjoyed your time with us.\n\nNow, are you ready to try out your new skills? Now would be a great time to join our [Housing Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) Getting Started competition. We've even prepared a [Bonus Lesson](https://www.kaggle.com/ryanholbrook/feature-engineering-for-house-prices) that collects all the work we've done together into a starter notebook.\n\n# References #\nHere are some great resources you might like to consult for more information. They all played a part in shaping this course:\n- *The Art of Feature Engineering*, a book by Pablo Duboue.\n- *An Empirical Analysis of Feature Engineering for Predictive Modeling*, an article by Jeff Heaton.\n- *Feature Engineering for Machine Learning*, a book by Alice Zheng and Amanda Casari. The tutorial on clustering was inspired by this excellent book.\n- *Feature Engineering and Selection*, a book by Max Kuhn and Kjell Johnson.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/feature-engineering/discussion) to chat with other learners.*","metadata":{}}]}